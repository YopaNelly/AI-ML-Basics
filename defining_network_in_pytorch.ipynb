{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG2S99G3FzB7D2bsy3H3YR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YopaNelly/AI-ML-Basics/blob/main/defining_network_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VRYliqfyVsmY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Create layers. In this case just a standard MLP\n",
        "    self.fc1 = nn.Linear(20, 10)\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "    self.fc2 = nn.Linear(10, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Call the layers in the appropriate sequence\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that at any time you can call your model like this:"
      ],
      "metadata": {
        "id": "khZk1hiIcdYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make up some data\n",
        "x = torch.rand(20)\n",
        "\n",
        "m = MyModel()\n",
        "out = m(x)"
      ],
      "metadata": {
        "id": "joi9-uoycbSZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is useful when developing your own architecture because you can verify that the model runs (for example, you got all the shapes right) and also you can check the shape of the output.\n",
        "Using nn.Sequential\n",
        "\n",
        "When the network is just a simple sequential application of the different layers, you can use nn.Sequential, which allows saving a lot of boilerplate code. For example, the previous network can be written as:"
      ],
      "metadata": {
        "id": "uFrY9RZwcgnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Create layers. In this case just a standard MLP\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Linear(20, 10),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(10, 3)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # nn.Sequential will call the layers\n",
        "    # in the order they have been inserted\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "1xs1zAV_cjKr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, when creating a network like this, we can skip the definition of a custom class and just use nn.Sequential like this:"
      ],
      "metadata": {
        "id": "LYwbX3-zcnTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(...)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "WYkE9QStcpv7",
        "outputId": "a7a3dc2a-48fd-4740-af91-b2294c946570"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ellipsis is not a Module subclass",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f1d369760271>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[misc, type-var]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36madd_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \"\"\"\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{torch.typename(module)} is not a Module subclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module name should be a string. Got {torch.typename(name)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ellipsis is not a Module subclass"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first method (defining a custom class) is more flexible and allows the use of architectures that are not strictly sequential. Therefore, we will use it throughout this class. However, the second abbreviated form is useful in many real-world circumstances.\n",
        "ReLU Activation Function\n",
        "\n",
        "The purpose of an activation function is to scale the outputs of a layer so that they are consistent, small values. Much like normalizing input values, this step ensures that our model trains efficiently!\n",
        "\n",
        "A ReLU activation function stands for \"Rectified Linear Unit\" and is one of the most commonly used activation functions for hidden layers. It is an activation function, simply defined as the positive part of the input, x. So, for an input image with any negative pixel values, this would turn all those values to 0, black. You may hear this referred to as \"clipping\" the values to zero; meaning that is the lower bound.\n",
        "Graph showing the Rectified Linear Unit (ReLU) activation function, which outputs zero for any negative input and outputs the input itself for non-negative input, represented by a horizontal line at zero merging into an upward sloping line.\n",
        "\n",
        "Rectified Linear Unit (ReLU) activation function\n",
        "Design of an MLP - Rules of Thumb\n",
        "\n",
        "When designing an MLP you have a lot of different possibilities, and it is sometimes hard to know where to start. Unfortunately there are no strict rules, and experimentation is key. However, here are some guidelines to help you get started with an initial architecture that makes sense, from which you can start experimenting.\n",
        "\n",
        "The number of inputs input_dim is fixed (in the case of MNIST images for example it is 28 x 28 = 784), so the first layer must be a fully-connected layer (Linear in PyTorch) with input_dim as input dimension.\n",
        "\n",
        "Also the number of outputs is fixed (it is determined by the desired outputs). For a classification problem it is the number of classes n_classes, and for a regression problem it is 1 (or the number of continuous values to predict). So the output layer is a Linear layer with n_classes (in case of classification).\n",
        "\n",
        "What remains to be decided is the number of hidden layers and their size. Typically you want to start from only one hidden layer, with a number of neurons between the input and the output dimension. Sometimes adding a second hidden layer helps, and in rare cases you might need to add more than one. But one is a good starting point.\n",
        "\n",
        "As for the number of neurons in the hidden layers, a decent starting point is usually the mean between the input and the output dimension. Then you can start experimenting with increasing or decreasing, and observe the performances you get. If you see overfitting(opens in a new tab), start by adding regularization (dropout(opens in a new tab) and weight decay) instead of decreasing the number of neurons, and see if that fixes it. A larger network with a bit of drop-out learns multiple ways to arrive to the right answer, so it is more robust than a smaller network without dropout. If this doesn't address the overfitting, then decrease the number of neurons. If you see underfitting(opens in a new tab), add more neurons. You can start by approximating up to the closest power of 2. Keep in mind that the number of neurons also depends on the size of your training dataset: a larger network is more powerful but it needs more data to avoid overfitting.\n",
        "\n",
        "So let's consider the MNIST classification problem. We have n_classes = 10 and input_dim = 784, so a starting point for our experimentation could be:"
      ],
      "metadata": {
        "id": "fEleayMkc3B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Create layers. In this case just a standard MLP\n",
        "    self.model = nn.Sequential(\n",
        "      # Input layer. The input is obviously 784. For\n",
        "      # the output (which is the input to the hidden layer)\n",
        "      # we take the mean between network input and output:\n",
        "      # (784 + 10) / 2 = 397 which we round to 400\n",
        "      nn.Linear(784, 400),\n",
        "      nn.Dropout(0.5),  # Combat overfitting\n",
        "      nn.ReLU(),\n",
        "      # Hidden layer\n",
        "      nn.Linear(400, 400),\n",
        "      nn.Dropout(0.5),  # Combat overfitting\n",
        "      nn.ReLU(),\n",
        "      # Output layer, must receive the output of the\n",
        "      # hidden layer and return the number of classes\n",
        "      nn.Linear(400, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # nn.Sequential will call the layers\n",
        "    # in the order they have been inserted\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "Tq4cKLuzc1uZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}